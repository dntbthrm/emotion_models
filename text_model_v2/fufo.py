from transformers import BertTokenizer, BertForSequenceClassification
import torch
import numpy as np

# ‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
model_dir = "emotion_bert_model"
#model_dir = "saved_rubert_model"
model = BertForSequenceClassification.from_pretrained(model_dir)
tokenizer = BertTokenizer.from_pretrained(model_dir)
model.eval()

# ‚úÖ –°–ª–æ–≤–∞—Ä—å –∫–ª–∞—Å—Å–æ–≤
basic_dict = {0: 'anger', 1: 'disgust', 2: 'fear', 3: 'joy', 4: 'sadness', 5: 'surprise', 6: 'neutral'}

# ‚úÖ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
def predict_emotion(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        predicted_class = torch.argmax(logits, dim=1).item()
        return basic_dict[predicted_class]

# ‚úÖ –ü—Ä–∏–º–µ—Ä
example_texts = ["–ü–æ—à–µ–ª –Ω–∞—Ö–µ—Ä!", "–Ø –æ—á–µ–Ω—å —Å–∫—É—á–∞—é –ø–æ —Ç–µ–±–µ, –º–Ω–µ –ø–ª–æ—Ö–æ", "–ù—É –∏ –≥–∞–¥–æ—Å—Ç—å, –º–µ–Ω—è —Ç–æ—à–Ω–∏—Ç –æ—Ç –±—Ä–æ–∫–∫–æ–ª–∏", "–í–∞—É, –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å!", "–Ø —Ä–∞–¥–æ —á—Ç–æ —Ç—ã –∑–¥–µ—Å—å. –°–ø–∞—Å–∏–±–æ", "–Ø —Ö–æ–¥–∏–ª–∞ –≤ –º–∞–≥–∞–∑–∏–Ω —Å–µ–≥–æ–¥–Ω—è"]
for txt in example_texts:
    predicted = predict_emotion(txt)
    print(f"üí¨ –¢–µ–∫—Å—Ç: {txt}\nüîÆ –≠–º–æ—Ü–∏—è: {predicted}")
